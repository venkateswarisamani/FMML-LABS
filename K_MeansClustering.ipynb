{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkateswarisamani/FMML-LABS/blob/main/K_MeansClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSVIRwShuG11"
      },
      "source": [
        "Lab by Ganesh Chandan\n",
        "\n",
        "kanakala.ganesh@research.iiit.ac.in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWsDgHdl5dOG"
      },
      "source": [
        "# K-Means\n",
        "\n",
        "K-Means algorithm is a centroid based clustering algorithm where the sum of distances of points from the centroid of each cluster is minimized. The final output is a set of K clusters .ie. the cluster assigned to each point and the K centroids of the clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smmi2Hm_6dtJ"
      },
      "source": [
        "### The Algorithm\n",
        "> 1.   Initialize K centroids to K points randomly and set each point's initial cluster as the centroid closest to it.\n",
        "2.   While the clusers are changing - \\\\\n",
        "a. Assign the new centroids as the centroids of the points which have the same assigned cluster. \\\\\n",
        "b. Assign the new clusters to the points as the closest centroid.\n",
        "3. Return the assignments and the centroids.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQWLS_wq_WVt"
      },
      "source": [
        "### Part 1 - Implementation of KMeans and testing on Synthetic Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYTNMyeL8rKH"
      },
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGIO_edJ4uhB"
      },
      "source": [
        "# Generating Synthetic Data\n",
        "\n",
        "k=2\n",
        "color=[\"darkturquoise\",\"darkorange\",\"teal\",\"darkviolet\",\"tomato\", \"yellowgreen\",\"hotpink\",\"gold\"]\n",
        "\n",
        "X,_=make_blobs(n_samples=500, n_features=2, centers=k, random_state=10)\n",
        "print(\"Shape = \" + str(X.shape))\n",
        "\n",
        "print(\"Sample:\")\n",
        "print(X[:7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Define all the functions & varibles needed to do clustering"
      ],
      "metadata": {
        "id": "WVLsu-efLqIk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHfSomXy91rb"
      },
      "source": [
        "#Create an empty dictionary to store the cluster name, centroid and points associated with the cluster.\n",
        "clusters={}\n",
        "\n",
        "def initializeClusterCentroids():\n",
        "  #For each cluster, initialize its properties\n",
        "  for i in range(k):\n",
        "      center = np.random.randint(-8, 10, size= (X.shape[1],), )\n",
        "      points = []\n",
        "\n",
        "      cluster = {\n",
        "          'center' : center,\n",
        "          'points' : points,\n",
        "          'color' : color[i]\n",
        "      }\n",
        "      clusters[i] = cluster #looping over clusters dictionary\n",
        "\n",
        "#Calculate euclidean distance between v1 and v2\n",
        "def distance(v1, v2):\n",
        "    return np.sum((v2-v1)**2)**0.5\n",
        "\n",
        "def assignPointsToCluster(clusters, X):\n",
        "    for ix in range(X.shape[0]):#iterate for all datapoints\n",
        "\n",
        "        #Foe each datapoint, find the distance to k centroids\n",
        "        distance_of_i = []\n",
        "        for kx in range(k):\n",
        "            d = distance(X[ix], clusters[kx]['center']) #we have find distance b/w\n",
        "            #all the point with all the five (k) cluster centers\n",
        "            distance_of_i.append(d)\n",
        "\n",
        "        #Based on the distances to k centroid, pick which cluster to assign it to\n",
        "        #here we want find the minimum distance of cluster centers\n",
        "        cluster_to_choose = np.argmin(distance_of_i)\n",
        "        clusters[cluster_to_choose]['points'].append(X[ix])\n",
        "\n",
        "#STEP 3: UPDATE CLUSTER CENTROIDS\n",
        "def updateCluster(clusters):\n",
        "    for kx in range(k):\n",
        "        cluster_points = clusters[kx]['points']\n",
        "\n",
        "        cluster_points = np.array(cluster_points)\n",
        "        #here pts are array of list but we want array of array so\n",
        "\n",
        "        if len(cluster_points)>0:\n",
        "            new_center = np.mean(cluster_points, axis=0) #axis is along rows so will\n",
        "            #find mean of all the feature seprately it will give 2,0 when we have cluster pts 50,2\n",
        "            clusters[kx]['center'] = new_center\n",
        "            clusters[kx]['points'] = [] # clear the points in a cluster list (emptying pts)\n",
        "            #as we have to do step 2 after step 3 again till not converge"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H95Y7_Es-5pl"
      },
      "source": [
        "#We need to visualize this too. Lets define a function that can do that!\n",
        "def plotclusters(clusters, plot):\n",
        "    for kx in range(k):\n",
        "        cluster_points = clusters[kx]['points']\n",
        "        cluster_color = clusters[kx]['color']\n",
        "        cluster_center = clusters[kx]['center']\n",
        "        cluster_points = np.array(cluster_points)\n",
        "\n",
        "        #plotting points associated /nearest to the cluster centers\n",
        "        if len(cluster_points) > 0:\n",
        "          plot.scatter(cluster_points[:, 0], cluster_points[:, 1], s = 2, c = cluster_color)\n",
        "\n",
        "        plot.scatter(cluster_center[0], cluster_center[1], s = 250, c = cluster_color, marker=\"o\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxdYCXL9_2Ol"
      },
      "source": [
        "We can now run and visualize how the clusters evolve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcTPXjLe-7nC"
      },
      "source": [
        "#Plotting the initial dataset\n",
        "fig, axs = plt.subplots(2, 3)\n",
        "fig.set_size_inches(15, 10)\n",
        "axs[0, 0].scatter(X[:,0],X[:,1], s = 2, c = 'black')\n",
        "axs[0, 0].set_title('Dataset')\n",
        "\n",
        "np.random.seed(1)\n",
        "initializeClusterCentroids()\n",
        "\n",
        "#Now, lets plot the centroids it has initalized\n",
        "axs[0, 1].set_title('Centroid Initialization')\n",
        "axs[0, 1].scatter(X[:, 0], X[:, 1], s = 2, c = 'black')\n",
        "for i in range(k):#for all the clusters\n",
        "    center = clusters[i]['center'] #its cluster's center\n",
        "    axs[0, 1].scatter(center[0], center[1], c=clusters[i]['color'], s=250, marker=\"o\")\n",
        "\n",
        "#CLUSTERING EPOCH 1\n",
        "axs[0, 2].set_title('Iteration 1')\n",
        "assignPointsToCluster(clusters, X)\n",
        "plotclusters(clusters, axs[0, 2])\n",
        "updateCluster(clusters)\n",
        "\n",
        "#CLUSTERING EPOCH 2\n",
        "axs[1, 0].set_title('Iteration 2')\n",
        "assignPointsToCluster(clusters, X)\n",
        "plotclusters(clusters, axs[1, 0])\n",
        "updateCluster(clusters)\n",
        "\n",
        "#CLUSTERING EPOCH 3\n",
        "axs[1, 1].set_title('Iteration 3')\n",
        "assignPointsToCluster(clusters, X)\n",
        "plotclusters(clusters, axs[1, 1])\n",
        "updateCluster(clusters)\n",
        "\n",
        "#CLUSTERING EPOCH 4\n",
        "axs[1, 2].set_title('Iteration 4')\n",
        "assignPointsToCluster(clusters, X)\n",
        "plotclusters(clusters, axs[1, 2])\n",
        "updateCluster(clusters)\n",
        "\n",
        "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "for ax in axs.flat:\n",
        "    ax.label_outer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqWdqALtALKg"
      },
      "source": [
        "Let us now try the same thing with different number of clusters. We will also see how initialization matters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDwkJxaP_BVA"
      },
      "source": [
        "# Generating Synthetic Data\n",
        "\n",
        "k=4\n",
        "color=[\"darkturquoise\",\"darkorange\",\"teal\",\"darkviolet\",\"tomato\", \"yellowgreen\",\"hotpink\",\"gold\"]\n",
        "\n",
        "X,_=make_blobs(n_samples=500, n_features=2, centers=k, random_state=10)\n",
        "print(\"Shape = \" + str(X.shape))\n",
        "\n",
        "print(\"Sample:\")\n",
        "print(X[:7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_HmX5BaAfjq"
      },
      "source": [
        "#Plotting the initial dataset\n",
        "fig, axs = plt.subplots(2, 3)\n",
        "fig.set_size_inches(15, 10)\n",
        "axs[0, 0].scatter(X[:,0],X[:,1], s = 2, c = 'black')\n",
        "axs[0, 0].set_title('Dataset')\n",
        "\n",
        "## Change initialization here to 'np.random.seed(0)'\n",
        "np.random.seed(3)\n",
        "initializeClusterCentroids()\n",
        "\n",
        "#Now, lets plot the centroids it has initalized\n",
        "axs[0, 1].set_title('Centroid Initialization')\n",
        "axs[0, 1].scatter(X[:, 0], X[:, 1], s = 2, c = 'black')\n",
        "for i in range(k):#for all the clusters\n",
        "    center = clusters[i]['center'] #its cluster's center\n",
        "    axs[0, 1].scatter(center[0], center[1], c=clusters[i]['color'], s=250, marker=\"o\")\n",
        "\n",
        "#CLUSTERING EPOCH 1\n",
        "axs[0, 2].set_title('Iteration 1')\n",
        "assignPointsToCluster(clusters, X)\n",
        "plotclusters(clusters, axs[0, 2])\n",
        "updateCluster(clusters)\n",
        "\n",
        "#CLUSTERING EPOCH 2\n",
        "axs[1, 0].set_title('Iteration 2')\n",
        "assignPointsToCluster(clusters, X)\n",
        "plotclusters(clusters, axs[1, 0])\n",
        "updateCluster(clusters)\n",
        "\n",
        "#CLUSTERING EPOCH 3\n",
        "axs[1, 1].set_title('Iteration 3')\n",
        "assignPointsToCluster(clusters, X)\n",
        "plotclusters(clusters, axs[1, 1])\n",
        "updateCluster(clusters)\n",
        "\n",
        "#CLUSTERING EPOCH 4\n",
        "axs[1, 2].set_title('Iteration 4')\n",
        "assignPointsToCluster(clusters, X)\n",
        "plotclusters(clusters, axs[1, 2])\n",
        "updateCluster(clusters)\n",
        "\n",
        "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
        "for ax in axs.flat:\n",
        "    ax.label_outer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlqe_wvnAyvp"
      },
      "source": [
        "**As you can see, changing the random seed and hence changing the initialization has a large effect on the performance of KMeans.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVyJPKobClt8"
      },
      "source": [
        "Hence we can run our K-Means Algorithm by calling the following function with appropriate X (dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PHM6J-lAg6E"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def run_kmeans(k, X):\n",
        "  initializeClusterCentroids()\n",
        "  assignPointsToCluster(clusters, X)\n",
        "  while (True):\n",
        "    prev_clusters = deepcopy(clusters)\n",
        "    updateCluster(clusters)\n",
        "    assignPointsToCluster(clusters, X)\n",
        "    if prev_clusters == clusters:\n",
        "      break\n",
        "  return clusters"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUxLYV43D99i"
      },
      "source": [
        "### Kmeans in scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVCkiXmdEcNg"
      },
      "source": [
        "Even though implementation of KMeans is easy, using libraries like sklearn allows our code to  be much simpler and faster. The next two cells show how to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT-WZDaikBD1"
      },
      "source": [
        "Check out all functionalities at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing Kmeans from scikit-learn\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "6v60U4zKUODb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP0Isa2gD7n3"
      },
      "source": [
        "kmeans_demo = KMeans(n_clusters=5) #random centroid intialization\n",
        "#kmeans_demo = KMeans() #What if we dont initialize the number of clusters?\n",
        "#kmeans_demo = KMeans(n_clusters=5, init='k-means++') #Does smart centroid initialization help?\n",
        "\n",
        "#Let us fit our synthetic datapoints into the kmeans object\n",
        "kmeans_demo.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHll4yqNFCSn"
      },
      "source": [
        "#Lets inspect the clustering model\n",
        "print(\"ITERATIONS TO CONVERGE = \" + str(kmeans_demo.n_iter_))\n",
        "print(\"CLUSTER CENTROIDS = \")\n",
        "print(str(kmeans_demo.cluster_centers_))\n",
        "print()\n",
        "\n",
        "#using the colours we defined for the clusters at the start\n",
        "colors_toplot = []\n",
        "for label in kmeans_demo.labels_:\n",
        "  colors_toplot.append(color[label])\n",
        "\n",
        "plt.scatter(X[:,0], X[:, 1], c = colors_toplot, s=2)\n",
        "plt.scatter(kmeans_demo.cluster_centers_[:,0], kmeans_demo.cluster_centers_[:,1], c = 'black', s=250, marker = \"o\")\n",
        "plt.gcf().set_size_inches(8,6)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBe2HWoNFWTO"
      },
      "source": [
        "### KMeans on Digits Dataset (KMeans for labelling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhekZruK9KNr"
      },
      "source": [
        "Here we will attempt to use k-means to try to identify similar digits without using the original label information; this might be similar to a first step in extracting meaning from a new dataset about which you don't have any a priori label information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4yV9wx9FVun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6826554-a40a-4a17-d57a-ff2fb287c14f"
      },
      "source": [
        "# importing the dataset\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "digits.data.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUBgYLeN9Sa-"
      },
      "source": [
        "# importing Kmeans from scikit-learn\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Perform KMeans\n",
        "kmeans = KMeans(n_clusters=10, random_state=0)\n",
        "clusters = kmeans.fit_predict(digits.data)\n",
        "kmeans.cluster_centers_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSVbUHUi9n18"
      },
      "source": [
        "# We can now see how the centers of the clusters formed are\n",
        "\n",
        "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
        "centers = kmeans.cluster_centers_.reshape(10, 8, 8)\n",
        "for axi, center in zip(ax.flat, centers):\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAMCCXBR-ARJ"
      },
      "source": [
        "Because k-means knows nothing about the identity of the cluster, the 0–9 labels may be permuted. We can fix this by matching each learned cluster label with the true labels found in them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E4d3Jkc-BMY"
      },
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "labels = np.zeros_like(clusters)\n",
        "for i in range(10):\n",
        "    mask = (clusters == i)\n",
        "    labels[mask] = mode(digits.target[mask])[0]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c2izOVb-Ezq"
      },
      "source": [
        "Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ2HJXhT-IJn",
        "outputId": "51c5701a-6009-4abf-bafb-2f082cd4c9c6"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(digits.target, labels)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7935447968836951"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwjdViYU-QRE"
      },
      "source": [
        "**Hence clustering algorithms like KMeans are highly effective for identifying labels for groups apriori and even a simple KMeans acheives a ~80% accuracy on digit classification.**\n",
        "\n",
        "The confusion matrix for this looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1idrQbsg-OQZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "mat = confusion_matrix(digits.target, labels)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=digits.target_names,\n",
        "            yticklabels=digits.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV0GTurvFkji"
      },
      "source": [
        "### KMeans on Iris Dataset (Elbow Method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNrXyKIjAp2L"
      },
      "source": [
        "We will perform KMeans on the famous Iris Dataset which has four features: sepal length, sepal width, petal length, and petal width. The fifth column is for species, which holds the value for these types of plants."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOcJbmeaFFhF"
      },
      "source": [
        "#Importing the neccesary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJWkYsYtA9Bi"
      },
      "source": [
        "#import the dataset\n",
        "iris = datasets.load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
        "x = df.iloc[:, [0,1,2,3]].values\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQAJZu30D9yx"
      },
      "source": [
        "Let us start with assigning the initial number of clusters as k=5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZKt6zzmD1e6"
      },
      "source": [
        "kmeans5 = KMeans(n_clusters=5)\n",
        "y_kmeans5 = kmeans5.fit_predict(x)\n",
        "print(y_kmeans5)\n",
        "\n",
        "kmeans5.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL3AAUdCEGsA"
      },
      "source": [
        "How do we choose the optimal number of clusters? There’s a method called the Elbow method, which is designed to help find the optimal number of clusters in a dataset. So let’s use this method to calculate the optimum value of k. To implement the Elbow method, we need to create some Python code (shown below), and we’ll plot a graph between the number of clusters and the corresponding error value.\n",
        "\n",
        "This graph generally ends up shaped like an elbow, hence its name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VSBRjiLEQ7V"
      },
      "source": [
        "Error =[]\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters = i).fit(x)\n",
        "    kmeans.fit(x)\n",
        "    Error.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1, 11), Error)\n",
        "plt.title('Elbow method')\n",
        "plt.xlabel('No of clusters')\n",
        "plt.ylabel('Error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEhXXxe6Em4E"
      },
      "source": [
        "The output graph of the Elbow method shows that the shape of elbow is approximately formed at k=3. The optimal value of k is between 2 and 4, as the elbow-like shape is formed at k=3 in the above graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTpdV03VE79J"
      },
      "source": [
        "kmeans3 = KMeans(n_clusters=3)\n",
        "y_kmeans3 = kmeans3.fit_predict(x)\n",
        "print(y_kmeans3)\n",
        "\n",
        "kmeans3.cluster_centers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_wPmjOSFP5y"
      },
      "source": [
        "We can now visualize the clustering in two dimensions instead of the original four."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOPhGuicFBkV"
      },
      "source": [
        "plt.scatter(x[:,0], x[:,1], c=y_kmeans3, cmap='rainbow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJz6aYt6RRRl"
      },
      "source": [
        "## Points to think about"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFLzASTIRaNc"
      },
      "source": [
        "1. Is feature scaling essential for KMeans as it is for most ML algos?                                                            \n",
        "(ans): Feature scaling is recommended for KMeans, especially if the features have different scales. While KMeans itself is not inherently affected by varying feature scales, it measures distances between data points, and features with larger scales might dominate the distance calculation. Therefore, scaling features can help ensure that all features contribute equally to the clustering process\n",
        "\n",
        "\n",
        "2. How can we prevent initialization variation in KMeans?\n",
        "(ans): Initialization variation in KMeans refers to the sensitivity of the algorithm's final clustering solution to the initial placement of cluster centroids. To mitigate this, several strategies can be employed:\n",
        "\n",
        "Multiple initializations: Run KMeans multiple times with different initializations and choose the clustering solution with the lowest inertia (sum of squared distances within clusters).\n",
        "KMeans++ initialization: Instead of randomly selecting initial centroids, use the KMeans++ algorithm, which selects initial centroids that are far apart from each other, leading to more stable clustering solutions.\n",
        "Adjusting convergence criteria: Modify the convergence criteria to allow for more iterations or stricter convergence thresholds to ensure convergence to a stable solution.\n",
        "\n",
        "3. What is the training and testing complexity of KMeans?              \n",
        "(ans):The training complexity of KMeans is typically\n",
        "�\n",
        "(\n",
        "�\n",
        "⋅\n",
        "�\n",
        "⋅\n",
        "�\n",
        "⋅\n",
        "�\n",
        ")\n",
        "O(k⋅n⋅d⋅i), where:\n",
        "\n",
        "�\n",
        "k is the number of clusters.\n",
        "�\n",
        "n is the number of data points.\n",
        "�\n",
        "d is the number of dimensions (features).\n",
        "�\n",
        "i is the number of iterations required for convergence.\n",
        "The testing complexity is\n",
        "�\n",
        "(\n",
        "�\n",
        "⋅\n",
        "�\n",
        "⋅\n",
        "�\n",
        ")\n",
        "O(k⋅d⋅t), where\n",
        "�\n",
        "t is the number of test data points. Testing complexity involves assigning each test data point to its nearest centroid, which requires calculating distances in the feature space for each centroid.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bwpt9K23ALRo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZrTx2nHSSZn"
      },
      "source": [
        "## References and resources\n",
        "\n",
        "1. https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n",
        "2. https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/\n",
        "3. https://en.wikipedia.org/wiki/K-means_clustering\n",
        "4. https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n",
        "5. https://heartbeat.fritz.ai/k-means-clustering-using-sklearn-and-python-4a054d67b187\n",
        "6. https://www.geeksforgeeks.org/k-means-clustering-introduction/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "az5_vhJnWLzS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}